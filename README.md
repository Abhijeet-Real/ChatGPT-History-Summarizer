
# ğŸ§  ChatGPT History Summarizer ğŸ”

This project enables fully local, customizable summarization of your exported ChatGPT conversation history â€” with the ability to extract, clean, chunk, and summarize only the most relevant content (e.g., Speech-to-Speech, STT, TTS, ASR pipelines).

---

## ğŸš€ Features

âœ… **Extracts** only messages relevant to your topic (e.g., S2S, Whisper, Bark, NeMo, etc.)  
âœ… **Cleans** and removes redundant code, markdown, or irrelevant meta-text  
âœ… **Chunks** cleaned content into manageable parts  
âœ… **Summarizes** using a 7B model (Qwen2-7B-GGUF) locally on your CPU/GPU  
âœ… **Hybrid Inference**: GPU handles 1 summary, rest run in parallel via CPU  
âœ… **Fully Modular**: Easy to plug in your own model or keywords

---

## ğŸ§© Folder Structure

```
History/
â”œâ”€â”€ keywords.ssf               # Your topic-specific keywords (e.g. tts, vad, whisper...)
â”œâ”€â”€ extracted history.ssf      # Auto-generated: filtered relevant messages
â”œâ”€â”€ cleaned history.ssf        # Auto-generated: cleaned for LLM
â”œâ”€â”€ chunk/                     # Auto-generated: chunked files for summarization
â””â”€â”€ summarize_chunk/           # Auto-generated: summary of each chunk
```

```
src/
â”œâ”€â”€ extract_history.py         # Extracts relevant messages from ChatGPT JSON
â”œâ”€â”€ clean_history.py           # Cleans up code blocks and noise
â”œâ”€â”€ create_chunk.py            # Chunks cleaned content (with overlap)
â”œâ”€â”€ QwenSummarizer.py          # Loads GGUF model using llama-cpp-python
â”œâ”€â”€ QwenDownloader.py          # Downloads model using huggingface_hub
â”œâ”€â”€ QwenLoader.py              # GPU/CPU hybrid model loading logic
â”œâ”€â”€ HuggingFaceLogin.py        # Handles token-based login
â”œâ”€â”€ history_pipeline.py        # One-click pipeline runner
â””â”€â”€ requirements.txt
```

---

## ğŸ›  How to Run

> âš ï¸ Requires 6 GB+ VRAM (GPU) and ~24 GB RAM (CPU parallel inference)

### 1. ğŸ§¹ Run the Full Pipeline
```bash
python src/history_pipeline.py
```

### 2. ğŸ“¦ Summarize (Hybrid CPU + GPU)
```bash
python src/run_summarizer.py
```

---

## ğŸ“‚ Input Format

Use ChatGPT's export (`conversations.json`)  
Place it in:  
```plaintext
OpenAI April 2025/conversations.json
```

---

## ğŸ”¥ Models

We use `Qwen2-7B-Instruct-GGUF` in quantized `Q4_K_M` format for blazing-fast local inference.

ğŸ“¦ Can be switched to any GGUF LLM using `QwenSummarizer.py`

---

## ğŸ§  Examples of Use

- Speech-to-Speech research logs ğŸ—£ï¸â†’ğŸ—£ï¸  
- Extracting just **NLP project** discussions from long chats  
- Cleaning and summarizing **case study solutions**  
- Building **personal knowledge base** from ChatGPT conversations

---

## ğŸ“œ License

MIT License  
Built by [Abhijeet](https://www.linkedin.com/in/abhijeet-099670300/) âš¡

---

## ğŸ’¬ Future Ideas

- [ ] Web UI (Streamlit-based)
- [ ] API summarization fallback
- [ ] Long context batching (for 100k+ token models)
